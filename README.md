# NeuralNetworks

Description:
-This repository houses a Python implementation of a trainable neural network designed for educational purposes and small-scale applications. Developed by Reece Gilbert, this neural network supports customizable input sizes, hidden layers, and output layers. It utilizes Rectified Linear Unit (ReLU) and hyperbolic tangent activation functions for hidden and output layers, respectively, along with Mean Squared Error (MSE) as the loss function for training.

Features:
-Flexible configuration of input size, number of hidden layers, and output neurons.
-Supports forward propagation, backpropagation with gradient descent for training.
-Activation functions include ReLU for hidden layers and tanh for output layer.
-Implements MSE loss function to evaluate and optimize network performance.

Usage:
-This network can be trained on various datasets for tasks such as regression or classification with continuous outputs. Example usage demonstrates training with random inputs to match predefined target values.

Future Development:
-Future updates may include enhancements such as additional activation functions, optimization algorithms, and compatibility with different data formats.
